import json
import numpy as np
import re
import requests
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import tokenizer_from_json
import math

# --- í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° ---
def load_tokenizer(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return tokenizer_from_json(json.load(f))

tokenizer_q = load_tokenizer('tokenizer_q.json')
tokenizer_a = load_tokenizer('tokenizer_a.json')

# --- ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ---
model = load_model('model.h5', compile=False)
# --- íŒŒë¼ë¯¸í„° ì„¤ì • ---
max_len_q = model.input_shape[0][1]
max_len_a = model.input_shape[1][1]
index_to_word = {v: k for k, v in tokenizer_a.word_index.items()}
index_to_word[0] = ''  # íŒ¨ë”© í† í°

start_token = 'start'
end_token = 'end'

# --- ìƒì„± ëª¨ë¸ ì‘ë‹µ (Greedy Search) ---
def decode_sequence_greedy(input_text):
    input_seq = tokenizer_q.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_len_q, padding='post')

    target_seq = tokenizer_a.texts_to_sequences([start_token])[0]
    target_seq = pad_sequences([target_seq], maxlen=max_len_a, padding='post')

    decoded_sentence = ''
    for i in range(max_len_a):
        predictions = model.predict([input_seq, target_seq], verbose=0)
        prob_dist = predictions[0, i, :]
        pred_id = np.argmax(prob_dist)  # ğŸ”¥ í•µì‹¬: ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ (Greedy!)

        pred_word = index_to_word.get(pred_id, '')
        if pred_word == end_token:
            break

        decoded_sentence += pred_word + ' '
        if i + 1 < max_len_a:
            target_seq[0, i + 1] = pred_id

    return re.sub(r'\s+', ' ', decoded_sentence).strip()

def extract_main_query(text):
    # ë¬¸ì¥ ë¶„ë¦¬ (".", "?" ê¸°ì¤€)
    sentences = re.split(r'[.?!]\s*', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return text

    # ë§ˆì§€ë§‰ ë¬¸ì¥ë§Œ ì„ íƒí•´ì„œ ì¡°ì‚¬ ì œê±°
    last = sentences[-1]

    # ê´„í˜¸/íŠ¹ìˆ˜ë¬¸ì ì œê±°
    last = re.sub(r'[^ê°€-í£a-zA-Z0-9 ]', '', last)

    # ì¡°ì‚¬ ì œê±°
    particles = ['ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ë³´ë‹¤']
    for p in particles:
        last = re.sub(rf'\b(\w+){p}\b', r'\1', last)

    return last.strip()


# --- ìœ„í‚¤ë°±ê³¼ ìš”ì•½ í¬ë¡¤ë§ ---
def get_wikipedia_summary(query):
    cleaned_query = extract_main_query(query)
    url = f"https://ko.wikipedia.org/api/rest_v1/page/summary/{cleaned_query}"
    res = requests.get(url)
    if res.status_code == 200:
        return res.json().get("extract", "ìš”ì•½ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        return "ìœ„í‚¤ë°±ê³¼ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# --- intent ë¶„ë¥˜ê¸° (ë£° ê¸°ë°˜) ---
def simple_intent_classifier(text):
    text = text.lower()
    greet_keywords = ["ì•ˆë…•", "ë°˜ê°€ì›Œ", "ì´ë¦„", "ëˆ„êµ¬", "ì†Œê°œ", "ì–´ë””ì„œ ì™”", "ì •ì²´", "ëª‡ ì‚´", "ë„ˆ ë­ì•¼"]
    info_keywords = ["ì„¤ëª…", "ì •ë³´", "ë¬´ì—‡", "ë­ì•¼", "ì–´ë””", "ëˆ„êµ¬", "ì™œ", "ì–´ë–»ê²Œ", "ì¢…ë¥˜", "ê°œë…"]
    math_keywords = ["ë”í•˜ê¸°", "ë¹¼ê¸°", "ê³±í•˜ê¸°", "ë‚˜ëˆ„ê¸°", "ë£¨íŠ¸", "ì œê³±", "+", "-", "*", "/", "=", "^", "âˆš", "ê³„ì‚°", "ëª‡ì´ì•¼", "ì–¼ë§ˆì•¼"]

    if any(kw in text for kw in greet_keywords):
        return "ì¸ì‚¬"
    elif any(kw in text for kw in info_keywords):
        return "ì •ë³´ì§ˆë¬¸"
    elif any(kw in text for kw in math_keywords):
        return "ìˆ˜í•™ì§ˆë¬¸"
    else:
        return "ì¼ìƒëŒ€í™”"


def parse_math_question(text):
    # ìì—°ì–´ â†’ ìˆ˜ì‹ìœ¼ë¡œ ê°„ë‹¨ ë³€í™˜
    text = text.replace("ê³±í•˜ê¸°", "*")
    text = text.replace("ë”í•˜ê¸°", "+")
    text = text.replace("ë¹¼ê¸°", "-")
    text = text.replace("ë‚˜ëˆ„ê¸°", "/")
    text = text.replace("ì œê³±", "**2")
    text = re.sub(r'ë£¨íŠ¸\s*(\d+)', r'math.sqrt(\1)', text)  # ë£¨íŠ¸ 9 â†’ math.sqrt(9)

    # ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸ í›„ ê³„ì‚°
    try:
        result = eval(text)
        return f"ì •ë‹µì€ {result}ì…ë‹ˆë‹¤."
    except:
        return "ê³„ì‚°í•  ìˆ˜ ì—†ëŠ” ìˆ˜ì‹ì´ì—ìš”. ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•´ ì£¼ì„¸ìš”!"


def is_math_question(text):
    return bool(re.search(r'[0-9]+|ê³±í•˜ê¸°|ë”í•˜ê¸°|ë‚˜ëˆ„ê¸°|ë¹¼ê¸°|ë£¨íŠ¸|ì œê³±', text))

def respond(input_text):
    intent = simple_intent_classifier(input_text)

    if "ì´ë¦„" in input_text:
        return "ì œ ì´ë¦„ì€ TriFusionNetì…ë‹ˆë‹¤."
    
    if "/ì‚¬ìš©ë²•" in input_text:
        return "\n ììœ ë¡­ê²Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”. \n ë”±íˆ ì œì•½ì€ ì—†ìŠµë‹ˆë‹¤."
        

    if "/help" in input_text:
        return "1. \n /model : ëª¨ë¸ ì†Œê°œ\n /detail : ëª¨ë¸ ì•„í‚¤í…ì³ \n /ì‚¬ìš©ë²•"

    if "/model" in input_text:
       return (
        "ì €ëŠ” TriFusionNetì´ë¼ëŠ” 2ê°œì˜ ë””ì½”ë”ì™€ ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ëŠ” Seq2Seq ì±—ë´‡ì…ë‹ˆë‹¤.\n"
        "ëª¨ë¸ ì•„í‚¤í…ì²˜ ìš”ì•½:\n"
        "- ì…ë ¥ ë ˆì´ì–´: 2ê°œì˜ ì…ë ¥ì„ ë°›ì•„ ê°ê¸° ë‹¤ë¥¸ ì„ë² ë”© ë ˆì´ì–´ë¡œ ë³€í™˜\n"
        "- ì¸ì½”ë”: LSTMì„ í†µí•´ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ê°ê¸° ë‹¤ë¥¸ Dense ë ˆì´ì–´ë¡œ ë³€í™˜\n"
        "- ë””ì½”ë”: 2ê°œì˜ LSTMì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•œ í›„ ê²°í•©\n"
        "- ë©€í‹°-í—¤ë“œ ì–´í…ì…˜: ì¸ì½”ë” ì¶œë ¥ê³¼ ë””ì½”ë” ì¶œë ¥ì„ ê²°í•©í•˜ì—¬ ì–´í…ì…˜ì„ ìˆ˜í–‰\n"
        "- ì¶œë ¥: TimeDistributed ë ˆì´ì–´ë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì¶œë ¥"
        "\nì´ ëª¨ë¸ì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë³µì¡í•œ ê´€ê³„ë¥¼ ì˜ í•™ìŠµí•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.\n ë” ìì„¸í•œ ì •ë³´ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ /detailì„ ì…ë ¥í•´ì£¼ì„¸ìš”!"
    )

    if "ëˆ„êµ¬" in input_text:
        return "ì €ëŠ” TriFusionNetì´ë¼ê³  í•´ìš”. "

    if intent == "ìˆ˜í•™ì§ˆë¬¸":
        return parse_math_question(input_text)

    if intent == "ì •ë³´ì§ˆë¬¸":
        keyword = re.sub(r"(ì— ëŒ€í•´|ì— ëŒ€í•œ|ì— ëŒ€í•´ì„œ)?\s*(ì„¤ëª…í•´ì¤˜|ì•Œë ¤ì¤˜|ë­ì•¼|ê°œë…|ì •ì˜|ì •ë³´)?", "", input_text).strip()
        if not keyword:
            return "ì–´ë–¤ ì£¼ì œì— ëŒ€í•´ ê¶ê¸ˆí•œê°€ìš”?"
        summary = get_wikipedia_summary(keyword)
        return f"{summary}\në‹¤ë¥¸ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹ ê°€ìš”?"
    else:
        return decode_sequence_greedy(input_text)

def auto_linebreak(text, max_length=80):
    """
    ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì— ì¤„ë°”ê¿ˆì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜.
    ì‰¼í‘œëŠ” ì¤„ë°”ê¿ˆí•˜ì§€ ì•ŠìŒ.
    """
    # 1. ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì— ì¤„ë°”ê¿ˆ
    text = re.sub(r'(?<=[.!?])\s+', '\n', text)

    # 2. ê¸¸ì´ê°€ ë„ˆë¬´ ê¸´ ì¤„ì€ ì ë‹¹íˆ ìë¥´ê¸° (ì„ íƒ)
    if max_length:
        lines = []
        for line in text.split('\n'):
            while len(line) > max_length:
                cut = line[:max_length].rfind(' ')
                if cut == -1:
                    cut = max_length
                lines.append(line[:cut])
                line = line[cut:].lstrip()
            lines.append(line)
        text = '\n'.join(lines)

    return text


# --- ëŒ€í™” ë£¨í”„ ---
if __name__ == "__main__":
    print("ğŸ¤– TriFusionNet ì±—ë´‡ì…ë‹ˆë‹¤! ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ /helpë¥¼ ì…ë ¥í•´ë³´ì„¸ìš” (ì¢…ë£Œ: 'exit')\n")

    while True:
        input_text = input("ğŸ‘¤ ë‹¹ì‹ : ")
        if input_text.lower() == 'exit':
            print("ğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break

        output_text = respond(input_text)
        formatted_output = auto_linebreak(output_text)  # â† ì—¬ê¸°ì„œ ì¤„ë°”ê¿ˆ ì ìš©!
        print("ğŸ¤– TriFusionNet:\n" + formatted_output)  # â† ë³´ê¸° ì¢‹ê²Œ ê°œí–‰í•´ì„œ ì¶œë ¥

